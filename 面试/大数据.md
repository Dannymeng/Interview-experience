# 大数据

## Hadoop

### **概念**
1. HDFS集群：负责海量数据的存储，集群中的角色主要有NameNode / DataNode / SecondaryNameNode
2. Yarn集群：负责海量数据运算时的资源调度，集群中的角色主要有 ResourceManager / NodeManager
3. MapReduce : 分布式计算框架

1. namenode：主要负责三个功能：1）管理元数据；2）维护目录树；3）响应客户请求
附：元数据（Metadata）：描述数据属性信息，用来指示存储位置、历史数据、资源查找、文件记录

2. - fsimage：是namenode中关于元数据的镜像，一般称为检查点
   - edits（Editlog）： 在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。Namenode在本地操作系统的文件系统中存储这个Editlog。设计出它的目的就是不想一改就全部修改FSImage内容，而是修改那些变动的内容；且在宕机时，也需要用它和前期的FSImage来创建内存中的FSImage
 <br><div align="center"><img width="320px" src="https://images0.cnblogs.com/i/587080/201405/221035025741273.png"></img></div>


3. MapReduce两种后台程序-jobtracker和tasktracker
- Jobtracker：Jobtracker是主线程，它负责接收客户作业提交，调度任务到工作节点上运行，并提供诸如监控工作节点状态及任务进度等管理功能，一个MapReduce集群有一个jobtracker，一般运行在可靠的硬件上
- tasktracker—由jobtracker指派任务，实例化用户程序，在本地执行任务并周期性地向jobtracker汇报状态。在每一个工作节点上永远只会有一个tasktracker。Tasktracker和DataNode运行在一个机器上，从而使得每一台物理机器既是一个计算节点，同时也是一个存储节点。每一个tasktracker能够配置map和reduce的任务片数（taskslot），这个数字代表每一种任务能被并行执行的数目

### **HDFS写数据流程**

1.  与namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在
2.  namenode返回是否可以上传
3.  client请求第一个 block该传输到哪些datanode服务器上
4.  namenode返回3个datanode服务器ABC
5.  client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端
6.  client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答
7.  client通知namenode存储块完毕， namenode将元数据同步到内存中。
8.  当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。

###  **HDFS读数据流程**

1.  跟namenode通信查询元数据，找到文件块所在的datanode服务器
2.  挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流
3.  datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）
4.  客户端以packet为单位接收，先在本地缓存，然后写入目标文件

### **元数据同步**

1. hadoop1.0使用secondarynamenode

 <br><div align="center"><img width="600px" src="https://github.com/Dannymeng/picture/blob/master/clipboard.png?raw=true"></img></div>

 <br><div align="center"><img width="600px" src="https://github.com/Dannymeng/picture/blob/master/clipboard2.png?raw=true"></img></div>

2. hadoop 2.X之后使用HA机制，QJM(Quorum Journal Manager)

active namenode会把最近的操作记录写到本地的一个edits文件中（edits file），并传输到NFS或者JN（journalnode）中。standby namenode定期的检查，从**NFS或者JN(journalnode)把最近的edit文件读过来，然后把edits文件和fsimage文件合并成一个新的fsimage**，合并完成之后会通知active namenode获取这个新fsimage。active namenode获得这个新的fsimage文件之后，替换原来旧的fsimage文件

active namenode和standby namenode之间是通过一组journalnode（数量是奇数，可以是3,5,7...,2n+1）来共享数据。active namenode把最近的edits文件写到2n+1个journalnode上，只要有n+1个写入成功就认为这次写入操作成功了，然后standby namenode就可以从journalnode上读取了。可以看到，QJM方式有容错的机制，可以容忍n个journalnode的失败。

### **MapReduce提价jar流程**

 <br><div align="center"><img width="600px" src="https://img-blog.csdn.net/20180824223513167?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDUzNTMyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></img></div>
1. 开发人员编写好MapReduce program，将程序打包运行
2. JobClient向JobTracker申请可用Job，JobTracker**返回JobClient一个可用Job ID**
3. JobClient得到Job ID后，将运行Job所需要的资源**拷贝到共享文件系统HDFS中**
4. 资源准备完备后，JobClient向JobTracker**提交Job**
5. JobTracker收到提交的Job后，**初始化Job**
6. 初始化完成后，JobTracker从HDFS中获取输入splits(作业应该启动多少Mapper任务)
7. **TaskTracker**不断地向JobTracker**汇报心跳信息**，并且返回要执行的任务
8. TaskTracker得到JobTracker分配(尽量满足数据本地化)的任务后，向HDFS获取Job资源(若数据是本地的，不需拷贝数据)
9. 获取资源后，TaskTracker会开启JVM子进程运行任务

[Hadoop三大组件学习](https://blog.csdn.net/weixin_40535323/article/details/82025442)

注：**3**中资源具体指什么？
主要包含：
-  程序jar包、作业配置文件xml
- 输入划分信息，决定作业该启动多少个map任务
- 本地文件，包含依赖的第三方jar包(-libjars)、依赖的归档文件(-archives)和普通文件(-files)，如果已经上传，则不需上传


## **Hadoop之分块（block）、分片（Splits）和 shuffle机制**

**分片(Splits)**
由**FileInputFormat**的getSplits方法。这里有一个新的概念：fileSplit。每个map处理一个fileSplit，**所以有多少个fileSplit就有多少个map**（map数并不是单纯的由用户设置决定的）。
```java
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));

long maxSize = getMaxSplitSize(job);

long blockSize = file.getBlockSize();

long splitSize = computeSplitSize(blockSize, minSize, maxSize);

　　protected long computeSplitSize(long blockSize, long minSize, long maxSize) {

　　　　return Math.max(minSize, Math.min(maxSize, blockSize));
　　　　
　　}
```

blockSize:默认为128M，默认情况下，切片大小=blocksize, 所以，**一般mapper的数量就是文件块的数量**。

不过这样设计也很有道理，因为块都是分散和副本存储的，所以可以参考块在哪个主机上就跟哪个主机分配map任务（不是唯一因素），实现本地性，提高效率。

**关于大量小文件的优化策略**
使用FileInputFormat衍生的**CombineFileInputFormat**将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量

**shuffing**

 <br><div align="center"><img width="700px" src="https://github.com/Dannymeng/picture/blob/master/mapreduce%E7%9A%84shuffle%E5%8E%9F%E7%90%86.png?raw=true"></img>

数据从map中出来到进入reduce之前称为shuffle阶段，shuffle的处理任务：将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序
1. maptask收集我们的map()方法输出的k、v对，放到内存缓冲区中
2. 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序
5. reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据
6. reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）
7. 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）

备注：Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整, 参数：io.sort.mb 默认100M



