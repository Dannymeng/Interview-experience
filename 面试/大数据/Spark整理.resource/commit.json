{"compress":true,"commitItems":[["303ef751-c5a4-4ec8-b46b-905362265694",1555590233316,"",[[1555590176007,["28447@DESKTOP-BE7HQ26",[[1,0,"# Spark整理\n\n\n\n"]],[0,0],[12,12]]],[1555590184810,["28447@DESKTOP-BE7HQ26",[[-1,12,"\n"],[1,13,"#"]],[12,12],[13,13]]],[1555590189973,["28447@DESKTOP-BE7HQ26",[[1,13,"# 基本概念"]],[13,13],[19,19]]],[1555590192481,["28447@DESKTOP-BE7HQ26",[[1,15,"Spark"]],[15,15],[20,20]]],[1555590193537,["28447@DESKTOP-BE7HQ26",[[1,24,"\n\n"]],[24,24],[25,25]]],[1555590196638,["28447@DESKTOP-BE7HQ26",[[-1,25,"\n"],[1,26,"-"]],[25,25],[26,26]]],[1555590202556,["28447@DESKTOP-BE7HQ26",[[1,26," 1. Sparl"]],[26,26],[35,35]]],[1555590203012,["28447@DESKTOP-BE7HQ26",[[-1,34,"l"]],[35,35],[34,34]]],[1555590205528,["28447@DESKTOP-BE7HQ26",[[1,34,"kContext"]],[34,34],[42,42]]],[1555590213746,["28447@DESKTOP-BE7HQ26",[[1,30,"**Application："],[-1,35,"Context"],[1,42,"应用程序**"]],[30,42],[55,55]]],[1555590223225,["28447@DESKTOP-BE7HQ26",[[1,55,"\n- 2. ****"]],[55,55],[63,63]]],[1555590224412,["28447@DESKTOP-BE7HQ26",[[1,63,"sajd "]],[63,63],[68,68]]],[1555590226907,["28447@DESKTOP-BE7HQ26",[[-1,63,"sajd "]],[68,68],[63,63]]],[1555590227726,["28447@DESKTOP-BE7HQ26",[[1,63,"ac"]],[63,63],[65,65]]],[1555590250919,["28447@DESKTOP-BE7HQ26",[[1,58,"\n- "]],[55,55],[58,58]]],[1555590251491,["28447@DESKTOP-BE7HQ26",[[1,56,"  "]],[58,58],[60,60]]],[1555590252253,["28447@DESKTOP-BE7HQ26",[[1,60,"指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。"]],[60,60],[117,117]]],[1555590264506,["28447@DESKTOP-BE7HQ26",[[-1,25,"- "]],[27,27],[25,25]]],[1555590268155,["28447@DESKTOP-BE7HQ26",[[-1,116,"- "]],[118,118],[116,116]]],[1555590272523,["28447@DESKTOP-BE7HQ26",[[-1,54,"  "]],[56,56],[54,54]]],[1555590273374,["28447@DESKTOP-BE7HQ26",[[1,54,"    "]],[54,54],[58,58]]],[1555590388883,["28447@DESKTOP-BE7HQ26",[[1,127,"\n3. "]],[127,127],[131,131]]],[1555590389247,["28447@DESKTOP-BE7HQ26",[[1,128,"  "]],[131,131],[133,133]]],[1555590390761,["28447@DESKTOP-BE7HQ26",[[-1,130,"3. "]],[133,133],[130,130]]],[1555590391846,["28447@DESKTOP-BE7HQ26",[[1,130,"  -"]],[130,130],[133,133]]],[1555590393322,["28447@DESKTOP-BE7HQ26",[[1,132,"  "]],[132,132],[134,134]]],[1555590394718,["28447@DESKTOP-BE7HQ26",[[1,135," "]],[135,135],[136,136]]],[1555590396931,["28447@DESKTOP-BE7HQ26",[[-1,132,"  "]],[134,134],[132,132]]],[1555590397995,["28447@DESKTOP-BE7HQ26",[[1,134," "]],[133,133],[134,134]]],[1555590398531,["28447@DESKTOP-BE7HQ26",[[1,134,"Spark中的Driver即运行上述Application的Main()函数并且创建SparkContext，其中创建SparkContext的目的是为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager通信，进行资源的申请、任务的分配和监控等;当Executor部分运行完毕后，Driver负责将SparkContext关闭。通常SparkContext代表Driver，如下图所示:"]],[134,134],[356,356]]],[1555590406992,["28447@DESKTOP-BE7HQ26",[[-1,123,"ac"],[1,125,"Driver：驱动程序"]],[121,127],[136,136]]],[1555590410966,["28447@DESKTOP-BE7HQ26",[[1,366,"\n    - "]],[366,366],[373,373]]],[1555590413958,["28447@DESKTOP-BE7HQ26",[[-1,371,"- "]],[373,373],[371,371]]],[1555590423027,["28447@DESKTOP-BE7HQ26",[[1,371,"![2.jpg](http://www.raincent.com/uploadfile/2018/0320/20180320013155532.jpg)\n    "]],[371,371],[452,452]]],[1555590423418,["28447@DESKTOP-BE7HQ26",[[-1,448,"    "],[1,452,"\n\n"]],[452,452],[449,449]]],[1555590425436,["28447@DESKTOP-BE7HQ26",[[-1,449,"\n"]],[449,449],[448,448]]],[1555590426062,["28447@DESKTOP-BE7HQ26",[[-1,448,"\n"],[1,449,"3"]],[448,448],[449,449]]],[1555590427272,["28447@DESKTOP-BE7HQ26",[[1,449,". "]],[449,449],[451,451]]],[1555590654871,["28447@DESKTOP-BE7HQ26",[[1,118,"    - \n"]],[117,117],[124,124]]],[1555590656240,["28447@DESKTOP-BE7HQ26",[[-1,122,"- "]],[124,124],[122,122]]],[1555590664134,["28447@DESKTOP-BE7HQ26",[[1,123,"![Application]($resource/Application.jpg)\n"]],[122,122],[165,165]]],[1555590731653,["28447@DESKTOP-BE7HQ26",[[-1,265,"为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager通信，进行资源的申请、任务的分配和监控等"]],[265,341],[265,265]]],[1555590733735,["28447@DESKTOP-BE7HQ26",[[1,265,"****"]],[265,265],[267,267]]],[1555590734286,["28447@DESKTOP-BE7HQ26",[[1,267,"为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager通信，进行资源的申请、任务的分配和监控等"]],[267,267],[343,343]]],[1555590750148,["28447@DESKTOP-BE7HQ26",[[1,502,"**Cluster Manager：资源管理器**\n4. "]],[502,502],[531,531]]],[1555590751079,["28447@DESKTOP-BE7HQ26",[[1,528,"  "]],[531,531],[533,533]]],[1555590752547,["28447@DESKTOP-BE7HQ26",[[-1,529," 4. "]],[533,533],[529,529]]],[1555590753203,["28447@DESKTOP-BE7HQ26",[[-1,528," "],[1,529,"\n"]],[529,529],[528,528]]],[1555590753924,["28447@DESKTOP-BE7HQ26",[[-1,528,"\n"],[1,529,"-"]],[528,528],[529,529]]],[1555590754532,["28447@DESKTOP-BE7HQ26",[[1,529,"  "]],[529,529],[531,531]]],[1555590757147,["28447@DESKTOP-BE7HQ26",[[1,528,"  "]],[528,528],[530,530]]],[1555590873994,["28447@DESKTOP-BE7HQ26",[[1,533,"指的是在集群上获取资源的外部服务，常用的有：Standalone，Spark原生的资源管理器，由Master负责资源的分配;Haddop Yarn，由Yarn中的ResearchManager负责资源的分配;Messos，由Messos中的Messos Master负责资源管理，如下图所示:"]],[533,533],[679,679]]],[1555590879767,["28447@DESKTOP-BE7HQ26",[[-1,598,"d"]],[599,599],[598,598]]],[1555590880160,["28447@DESKTOP-BE7HQ26",[[1,599,"o"]],[598,598],[599,599]]],[1555590884739,["28447@DESKTOP-BE7HQ26",[[-1,595,"Hadoop Yarn"]],[595,606],[595,595]]],[1555590885990,["28447@DESKTOP-BE7HQ26",[[1,595,"****"]],[595,595],[597,597]]],[1555590886587,["28447@DESKTOP-BE7HQ26",[[1,597,"Hadoop Yarn"]],[597,597],[608,608]]],[1555590889941,["28447@DESKTOP-BE7HQ26",[[-1,555,"Standalone"]],[555,565],[555,555]]],[1555590893464,["28447@DESKTOP-BE7HQ26",[[1,555,"**Standalone****"]],[555,555],[569,569]]],[1555590894799,["28447@DESKTOP-BE7HQ26",[[-1,569,"**"]],[569,569],[567,567]]],[1555590903071,["28447@DESKTOP-BE7HQ26",[[1,687,"\n  - "]],[687,687],[692,692]]],[1555590904810,["28447@DESKTOP-BE7HQ26",[[-1,690,"- "]],[692,692],[690,690]]],[1555590909809,["28447@DESKTOP-BE7HQ26",[[1,690,"\n![Cluster Manager]($resource/Cluster%20Manager.jpg)\n\n"]],[690,690],[743,743]]],[1555590912245,["28447@DESKTOP-BE7HQ26",[[1,744,"\n"]],[743,743],[744,744]]],[1555590913909,["28447@DESKTOP-BE7HQ26",[[-1,744,"\n"]],[744,744],[743,743]]],[1555590914899,["28447@DESKTOP-BE7HQ26",[[-1,743,"\n"],[1,744,"4"]],[743,743],[744,744]]],[1555590921850,["28447@DESKTOP-BE7HQ26",[[1,744,". **Executor：执行器**\n5. "]],[744,744],[766,766]]],[1555590922220,["28447@DESKTOP-BE7HQ26",[[-1,763,"5. "],[1,766,"\n\n"]],[766,766],[764,764]]],[1555590922971,["28447@DESKTOP-BE7HQ26",[[-1,764,"\n"]],[764,764],[763,763]]],[1555590977137,["28447@DESKTOP-BE7HQ26",[[1,763,"  - Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立的一批Executor，如下图所示:"]],[763,763],[862,862]]],[1555590979675,["28447@DESKTOP-BE7HQ26",[[1,863,"  - \n"]],[862,862],[867,867]]],[1555590981183,["28447@DESKTOP-BE7HQ26",[[-1,865,"- "]],[867,867],[865,865]]],[1555590985150,["28447@DESKTOP-BE7HQ26",[[-1,530,"- "]],[532,532],[530,530]]],[1555590987821,["28447@DESKTOP-BE7HQ26",[[1,530,"\n   "]],[530,530],[534,534]]],[1555590991380,["28447@DESKTOP-BE7HQ26",[[-1,532,"   "]],[534,534],[531,531]]],[1555590991755,["28447@DESKTOP-BE7HQ26",[[-1,530,"\n"]],[531,531],[530,530]]],[1555590993178,["28447@DESKTOP-BE7HQ26",[[1,530,"-"]],[530,530],[531,531]]],[1555590994184,["28447@DESKTOP-BE7HQ26",[[1,532," "]],[531,531],[532,532]]],[1555591003739,["28447@DESKTOP-BE7HQ26",[[1,866,"![Executor]($resource/Executor.jpg)\n\n"]],[865,865],[902,902]]],[1555591006505,["28447@DESKTOP-BE7HQ26",[[1,903,"\n"]],[902,902],[903,903]]],[1555591007303,["28447@DESKTOP-BE7HQ26",[[-1,903,"\n"]],[903,903],[902,902]]],[1555591008414,["28447@DESKTOP-BE7HQ26",[[-1,902,"\n"],[1,903,"5"]],[902,902],[903,903]]],[1555591014867,["28447@DESKTOP-BE7HQ26",[[1,903,". **Worker：计算节点**"]],[903,903],[920,920]]],[1555591015936,["28447@DESKTOP-BE7HQ26",[[1,920,"\n6. "]],[920,920],[924,924]]],[1555591017217,["28447@DESKTOP-BE7HQ26",[[-1,921,"6. "],[1,924,"\n\n"]],[924,924],[922,922]]],[1555591018077,["28447@DESKTOP-BE7HQ26",[[-1,922,"\n"]],[922,922],[921,921]]],[1555591145802,["28447@DESKTOP-BE7HQ26",[[1,921,"集群中任何可以运行Application代码的节点，类似于Yarn中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示:"]],[921,921],[1088,1088]]],[1555591147785,["28447@DESKTOP-BE7HQ26",[[1,1089,"\n"]],[1088,1088],[1089,1089]]],[1555591171918,["28447@DESKTOP-BE7HQ26",[[1,1089,"![Worker]($resource/Worker.jpg)\n"]],[1089,1089],[1121,1121]]],[1555591174221,["28447@DESKTOP-BE7HQ26",[[1,1122,"\n"]],[1121,1121],[1122,1122]]],[1555591175906,["28447@DESKTOP-BE7HQ26",[[-1,1122,"\n"]],[1122,1122],[1121,1121]]],[1555591176161,["28447@DESKTOP-BE7HQ26",[[-1,1121,"\n"],[1,1122,"6"]],[1121,1121],[1122,1122]]],[1555591183944,["28447@DESKTOP-BE7HQ26",[[1,1122,". **RDD：弹性分布式数据集**\n7. "]],[1122,1122],[1144,1144]]],[1555591185509,["28447@DESKTOP-BE7HQ26",[[-1,1142,". "]],[1144,1144],[1142,1142]]],[1555591185989,["28447@DESKTOP-BE7HQ26",[[-1,1141,"7"],[1,1142,"\n"]],[1142,1142],[1141,1141]]],[1555591188567,["28447@DESKTOP-BE7HQ26",[[1,1121,"\n"]],[1120,1120],[1121,1121]]],[1555591190574,["28447@DESKTOP-BE7HQ26",[[1,902,"\n"]],[901,901],[902,902]]],[1555591194291,["28447@DESKTOP-BE7HQ26",[[1,743,"\n"]],[742,742],[743,743]]],[1555591196516,["28447@DESKTOP-BE7HQ26",[[1,499,"    \n"]],[498,498],[503,503]]],[1555591208570,["28447@DESKTOP-BE7HQ26",[[-1,770," - "]],[773,773],[770,770]]],[1555591209554,["28447@DESKTOP-BE7HQ26",[[1,770,"\n "]],[770,770],[772,772]]],[1555591211651,["28447@DESKTOP-BE7HQ26",[[-1,771," "]],[772,772],[771,771]]],[1555591212161,["28447@DESKTOP-BE7HQ26",[[-1,770,"\n"]],[771,771],[770,770]]],[1555591216223,["28447@DESKTOP-BE7HQ26",[[1,770,"      "]],[770,770],[776,776]]],[1555591220369,["28447@DESKTOP-BE7HQ26",[[1,931,"        "]],[931,931],[939,939]]],[1555591243897,["28447@DESKTOP-BE7HQ26",[[1,1160,"Resillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作(主要有Transformation和Action操作)，如下图所示:"]],[1160,1160],[1252,1252]]],[1555591245791,["28447@DESKTOP-BE7HQ26",[[1,1253,"\n"]],[1252,1252],[1253,1253]]],[1555591288824,["28447@DESKTOP-BE7HQ26",[[1,1253,"![RDD]($resource/RDD.jpg)\n"]],[1253,1253],[1279,1279]]],[1555591290511,["28447@DESKTOP-BE7HQ26",[[1,1280,"\n"]],[1279,1279],[1280,1280]]],[1555591291909,["28447@DESKTOP-BE7HQ26",[[-1,1280,"\n"],[1,1281,"7"]],[1280,1280],[1281,1281]]],[1555591304072,["28447@DESKTOP-BE7HQ26",[[1,1281,". \n![窄依赖]($resource/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg)\n\n"]],[1281,1281],[1334,1334]]],[1555591322556,["28447@DESKTOP-BE7HQ26",[[1,1283,"父RDD每一个分区最多被一个子RDD的分区所用;表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示:"]],[1283,1283],[1361,1361]]],[1555591328862,["28447@DESKTOP-BE7HQ26",[[-1,1283,"父RDD每一个分区最多被一个子RDD的分区所用"]],[1283,1306],[1283,1283]]],[1555591330086,["28447@DESKTOP-BE7HQ26",[[1,1283,"****"]],[1283,1283],[1285,1285]]],[1555591330444,["28447@DESKTOP-BE7HQ26",[[1,1285,"父RDD每一个分区最多被一个子RDD的分区所用"]],[1285,1285],[1308,1308]]],[1555591334576,["28447@DESKTOP-BE7HQ26",[[1,1366,"8. \n"]],[1365,1365],[1369,1369]]],[1555591338058,["28447@DESKTOP-BE7HQ26",[[-1,1366,"8. "]],[1369,1369],[1366,1366]]],[1555591341595,["28447@DESKTOP-BE7HQ26",[[-1,1251,":"]],[1252,1252],[1251,1251]]],[1555591344421,["28447@DESKTOP-BE7HQ26",[[1,1251,"："]],[1251,1251],[1252,1252]]],[1555591344869,["28447@DESKTOP-BE7HQ26",[[1,1253,"\n"]],[1252,1252],[1253,1253]]],[1555591348452,["28447@DESKTOP-BE7HQ26",[[1,1107,"        \n"]],[1106,1106],[1115,1115]]],[1555591352104,["28447@DESKTOP-BE7HQ26",[[-1,1107,"        "]],[1115,1115],[1107,1107]]],[1555591354305,["28447@DESKTOP-BE7HQ26",[[1,1420,"\n"]],[1418,1418],[1419,1419]]],[1555591355200,["28447@DESKTOP-BE7HQ26",[[1,1421,"\n"]],[1419,1419],[1420,1420]]],[1555591366231,["28447@DESKTOP-BE7HQ26",[[1,1420,"8. **宽依赖**\n\n父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示:"]],[1420,1420],[1482,1482]]],[1555591369999,["28447@DESKTOP-BE7HQ26",[[1,1484,"\n"]],[1482,1482],[1483,1483]]],[1555591370683,["28447@DESKTOP-BE7HQ26",[[1,1485,"\n"]],[1483,1483],[1484,1484]]],[1555591371484,["28447@DESKTOP-BE7HQ26",[[-1,1485,"\n"]],[1484,1484],[1483,1483]]],[1555591371925,["28447@DESKTOP-BE7HQ26",[[1,1485,"\n"]],[1483,1483],[1484,1484]]],[1555591375525,["28447@DESKTOP-BE7HQ26",[[1,1484,"![宽依赖]($resource/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg)"]],[1484,1484],[1534,1534]]],[1555591519178,["28447@DESKTOP-BE7HQ26",[[1,1535,"\n"]],[1534,1534],[1535,1535]]],[1555591519567,["28447@DESKTOP-BE7HQ26",[[-1,1535,"\n"],[1,1536,"9"]],[1535,1535],[1536,1536]]],[1555591520120,["28447@DESKTOP-BE7HQ26",[[1,1536," ."]],[1536,1536],[1538,1538]]],[1555591520809,["28447@DESKTOP-BE7HQ26",[[-1,1536," ."]],[1538,1538],[1536,1536]]],[1555591521342,["28447@DESKTOP-BE7HQ26",[[1,1536,"。 "]],[1536,1536],[1538,1538]]],[1555591522402,["28447@DESKTOP-BE7HQ26",[[-1,1536,"。 "]],[1538,1538],[1536,1536]]],[1555591523093,["28447@DESKTOP-BE7HQ26",[[1,1536,". "]],[1536,1536],[1538,1538]]],[1555591551085,["28447@DESKTOP-BE7HQ26",[[1,1535,"\n"]],[1534,1534],[1535,1535]]],[1555591551629,["28447@DESKTOP-BE7HQ26",[[1,1535,"常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join(父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖)。\n\n常见的宽依赖有groupByKey、partitionBy、reduceByKey、join(父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖)。"]],[1535,1535],[1805,1805]]],[1555591556095,["28447@DESKTOP-BE7HQ26",[[1,1806,"\n"]],[1806,1806],[1807,1807]]],[1555591565340,["28447@DESKTOP-BE7HQ26",[[1,1810,"**DAG：有向无环图**\n10. "]],[1810,1810],[1828,1828]]],[1555591566848,["28447@DESKTOP-BE7HQ26",[[-1,1825,"0. "]],[1828,1828],[1825,1825]]],[1555591567155,["28447@DESKTOP-BE7HQ26",[[-1,1824,"1"],[1,1825,"\n"]],[1825,1825],[1824,1824]]],[1555591568679,["28447@DESKTOP-BE7HQ26",[[1,1825,"\n"]],[1824,1824],[1825,1825]]],[1555591587604,["28447@DESKTOP-BE7HQ26",[[1,1824,"Directed Acycle graph，反应RDD之间的依赖关系，如图所示:"]],[1824,1824],[1864,1864]]],[1555591588994,["28447@DESKTOP-BE7HQ26",[[1,1866,"\n"]],[1864,1864],[1865,1865]]],[1555591597327,["28447@DESKTOP-BE7HQ26",[[1,1865,"![DAG]($resource/DAG.jpg)"]],[1865,1865],[1891,1891]]],[1555591603456,["28447@DESKTOP-BE7HQ26",[[1,1892,"\n"]],[1891,1891],[1892,1892]]],[1555591604659,["28447@DESKTOP-BE7HQ26",[[-1,1892,"\n"],[1,1893,"1"]],[1892,1892],[1893,1893]]],[1555591612482,["28447@DESKTOP-BE7HQ26",[[1,1893,"0. **DAGScheduler：有向无环图调度器**\n11. "]],[1893,1893],[1926,1926]]],[1555591613809,["28447@DESKTOP-BE7HQ26",[[-1,1923,"1. "]],[1926,1926],[1923,1923]]],[1555591614002,["28447@DESKTOP-BE7HQ26",[[-1,1922,"1"],[1,1923,"\n"]],[1923,1923],[1922,1922]]],[1555591625637,["28447@DESKTOP-BE7HQ26",[[1,1922,"基于DAG划分Stage 并以TaskSet的形势提交Stage给TaskScheduler;负责将作业拆分成不同阶段的具有依赖关系的多批任务;最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在SparkContext初始化的过程中被实例化，一个SparkContext对应创建一个DAGScheduler。"]],[1922,1922],[2083,2083]]],[1555591653000,["28447@DESKTOP-BE7HQ26",[[1,2084,"\n"]],[2083,2083],[2084,2084]]],[1555591653432,["28447@DESKTOP-BE7HQ26",[[1,2085,"\n"]],[2084,2084],[2085,2085]]],[1555591657989,["28447@DESKTOP-BE7HQ26",[[1,2084,"![DAGScheduler]($resource/DAGScheduler.jpg)"]],[2084,2084],[2128,2128]]],[1555591673591,["28447@DESKTOP-BE7HQ26",[[-1,1945,"形势"]],[1947,1947],[1945,1945]]],[1555591674148,["28447@DESKTOP-BE7HQ26",[[1,1945,"xin"]],[1945,1945],[1948,1948]]],[1555591675016,["28447@DESKTOP-BE7HQ26",[[-1,1945,"xin"]],[1948,1948],[1945,1945]]],[1555591676844,["28447@DESKTOP-BE7HQ26",[[1,1945,"形式"]],[1945,1945],[1947,1947]]],[1555591711692,["28447@DESKTOP-BE7HQ26",[[1,2129,"\n"]],[2128,2128],[2129,2129]]],[1555591711936,["28447@DESKTOP-BE7HQ26",[[-1,2129,"\n"],[1,2130,"1"]],[2129,2129],[2130,2130]]],[1555591719548,["28447@DESKTOP-BE7HQ26",[[1,2130,"1. **TaskScheduler：任务调度器**\n12. "]],[2130,2130],[2161,2161]]],[1555591719808,["28447@DESKTOP-BE7HQ26",[[-1,2157,"12. "],[1,2161,"\n\n"]],[2161,2161],[2158,2158]]],[1555591720436,["28447@DESKTOP-BE7HQ26",[[-1,2158,"\n"]],[2158,2158],[2157,2157]]],[1555591739626,["28447@DESKTOP-BE7HQ26",[[1,2157,"将Taskset提交给worker(集群)运行并回报结果;负责每个具体任务的实际物理调度。如图所示:"]],[2157,2157],[2207,2207]]],[1555591741284,["28447@DESKTOP-BE7HQ26",[[1,2208,"\n"]],[2207,2207],[2208,2208]]],[1555591780603,["28447@DESKTOP-BE7HQ26",[[1,2208,"![TaskScheduler]($resource/TaskScheduler.jpg)\n"]],[2208,2208],[2254,2254]]],[1555591782822,["28447@DESKTOP-BE7HQ26",[[1,2208,"\n"]],[2207,2207],[2208,2208]]],[1555591784872,["28447@DESKTOP-BE7HQ26",[[1,2256,"\n"]],[2255,2255],[2256,2256]]],[1555591785101,["28447@DESKTOP-BE7HQ26",[[-1,2256,"\n"],[1,2257,"1"]],[2256,2256],[2257,2257]]],[1555591785540,["28447@DESKTOP-BE7HQ26",[[1,2257,"2 "]],[2257,2257],[2259,2259]]],[1555591786005,["28447@DESKTOP-BE7HQ26",[[-1,2258," "]],[2259,2259],[2258,2258]]],[1555591786455,["28447@DESKTOP-BE7HQ26",[[1,2258,". "]],[2258,2258],[2260,2260]]],[1555591832409,["28447@DESKTOP-BE7HQ26",[[1,2260,"由一个或多个调度阶段所组成的一次计算作业;包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示:"]],[2260,2260],[2355,2355]]],[1555591835713,["28447@DESKTOP-BE7HQ26",[[1,2260,"\n"]],[2260,2260],[2261,2261]]],[1555591843306,["28447@DESKTOP-BE7HQ26",[[1,2260,"**Job：作业**"]],[2260,2260],[2270,2270]]],[1555591846357,["28447@DESKTOP-BE7HQ26",[[1,2366,"\n\n"]],[2366,2366],[2367,2367]]],[1555591856397,["28447@DESKTOP-BE7HQ26",[[1,2367,"![Job]($resource/Job.jpg)\n"]],[2367,2367],[2393,2393]]],[1555591858311,["28447@DESKTOP-BE7HQ26",[[1,2367,"\n"]],[2366,2366],[2367,2367]]],[1555591859279,["28447@DESKTOP-BE7HQ26",[[1,2395,"\n"]],[2393,2393],[2394,2394]]],[1555591859811,["28447@DESKTOP-BE7HQ26",[[1,2396,"\n"]],[2394,2394],[2395,2395]]],[1555591871494,["28447@DESKTOP-BE7HQ26",[[1,2395,"13. **Stage：调度阶段**\n\n一个任务集对应的调度阶段;每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段;Stage分成两种类型ShuffleMapStage、ResultStage。如图所示:"]],[2395,2395],[2524,2524]]],[1555591875634,["28447@DESKTOP-BE7HQ26",[[1,2526,"\n"]],[2524,2524],[2525,2525]]],[1555591881802,["28447@DESKTOP-BE7HQ26",[[1,2525,"![Stage]($resource/Stage.jpg)"]],[2525,2525],[2555,2555]]],[1555591909874,["28447@DESKTOP-BE7HQ26",[[1,2556,"\n"]],[2555,2555],[2556,2556]]],[1555591910080,["28447@DESKTOP-BE7HQ26",[[-1,2556,"\n"],[1,2557,"1"]],[2556,2556],[2557,2557]]],[1555591925115,["28447@DESKTOP-BE7HQ26",[[1,2557,"4. **TaskSet：任务集**\n\n由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示:"]],[2557,2557],[2618,2618]]],[1555591926997,["28447@DESKTOP-BE7HQ26",[[1,2618,"\n\n"]],[2618,2618],[2619,2619]]],[1555591933562,["28447@DESKTOP-BE7HQ26",[[1,2619,"![TaskSet]($resource/TaskSet.jpg)\n"]],[2619,2619],[2653,2653]]],[1555591935369,["28447@DESKTOP-BE7HQ26",[[1,2654,"\n"]],[2653,2653],[2654,2654]]],[1555591982738,["28447@DESKTOP-BE7HQ26",[[1,2654,"提示：\n\n1)一个Stage创建一个TaskSet;\n2)为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet"]],[2654,2654],[2722,2722]]],[1555591986126,["28447@DESKTOP-BE7HQ26",[[1,2723,"\n"]],[2722,2722],[2723,2723]]],[1555591986411,["28447@DESKTOP-BE7HQ26",[[1,2724,"\n"]],[2723,2723],[2724,2724]]],[1555592001178,["28447@DESKTOP-BE7HQ26",[[1,2724,"**15、Task：任务**\n\n被送到某个Executor上的工作任务;单个分区数据集上的最小处理流程单元。如图所示:"]],[2724,2724],[2783,2783]]],[1555592002746,["28447@DESKTOP-BE7HQ26",[[1,2784,"\n"]],[2783,2783],[2784,2784]]],[1555592002987,["28447@DESKTOP-BE7HQ26",[[1,2785,"\n"]],[2784,2784],[2785,2785]]],[1555592008649,["28447@DESKTOP-BE7HQ26",[[1,2785,"![Task]($resource/Task.jpg)\n"]],[2785,2785],[2813,2813]]],[1555592030048,["28447@DESKTOP-BE7HQ26",[[1,2814,"\n"]],[2813,2813],[2814,2814]]],[1555592030630,["28447@DESKTOP-BE7HQ26",[[1,2814,"总体如图所示："]],[2814,2814],[2821,2821]]],[1555592031757,["28447@DESKTOP-BE7HQ26",[[1,2822,"\n"]],[2821,2821],[2822,2822]]],[1555592061431,["28447@DESKTOP-BE7HQ26",[[1,2822,"![总体图]($resource/%E6%80%BB%E4%BD%93%E5%9B%BE.jpg)\n"]],[2822,2822],[2872,2872]]],[1555592164232,["28447@DESKTOP-BE7HQ26",[[1,2873,"\n"]],[2872,2872],[2873,2873]]],[1555592177302,["28447@DESKTOP-BE7HQ26",[[-1,2873,"\n"],[1,2874,"#"]],[2873,2873],[2874,2874]]],[1555592180985,["28447@DESKTOP-BE7HQ26",[[1,2874,"# Spark 与百姓"]],[2874,2874],[2885,2885]]],[1555592182642,["28447@DESKTOP-BE7HQ26",[[-1,2881," 与百姓"]],[2885,2885],[2881,2881]]],[1555592184012,["28447@DESKTOP-BE7HQ26",[[1,2881,"运行"]],[2881,2881],[2883,2883]]],[1555592186466,["28447@DESKTOP-BE7HQ26",[[1,2883,"流程"]],[2883,2883],[2885,2885]]],[1555592234015,["28447@DESKTOP-BE7HQ26",[[1,2885,"\n![Spark运行基本流程1]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B1.jpg)\n\n"]],[2885,2885],[2978,2978]]],[1555592241408,["28447@DESKTOP-BE7HQ26",[[1,2978,"![Spark运行基本流程2]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B2.jpg)\n"]],[2978,2978],[3070,3070]]],[1555592246676,["28447@DESKTOP-BE7HQ26",[[1,2978,"\n"]],[2977,2977],[2978,2978]]],[1555592249749,["28447@DESKTOP-BE7HQ26",[[1,3072,"\n"]],[3070,3070],[3071,3071]]],[1555593286834,["28447@DESKTOP-BE7HQ26",[[1,3072,"**四、Spark核心原理透视**"]],[3072,3072],[3089,3089]]],[1555593289717,["28447@DESKTOP-BE7HQ26",[[-1,3072,"**四"]],[3075,3075],[3072,3072]]],[1555593291513,["28447@DESKTOP-BE7HQ26",[[1,3072,"## "]],[3072,3072],[3075,3075]]],[1555593293812,["28447@DESKTOP-BE7HQ26",[[-1,3087,"**"]],[3089,3089],[3087,3087]]],[1555593295759,["28447@DESKTOP-BE7HQ26",[[-1,3075,"、"]],[3076,3076],[3075,3075]]],[1555593299362,["28447@DESKTOP-BE7HQ26",[[1,3087,"\n"]],[3086,3086],[3087,3087]]],[1555593300171,["28447@DESKTOP-BE7HQ26",[[1,3088,"\n"]],[3087,3087],[3088,3088]]],[1555593305788,["28447@DESKTOP-BE7HQ26",[[1,3088,"**1、计算流程**"]],[3088,3088],[3098,3098]]],[1555593308601,["28447@DESKTOP-BE7HQ26",[[1,3099,"\n"]],[3098,3098],[3099,3099]]],[1555593311265,["28447@DESKTOP-BE7HQ26",[[-1,3091,"、"]],[3092,3092],[3091,3091]]],[1555593311609,["28447@DESKTOP-BE7HQ26",[[1,3091,"。"]],[3091,3091],[3092,3092]]],[1555593312442,["28447@DESKTOP-BE7HQ26",[[-1,3091,"。"]],[3092,3092],[3091,3091]]],[1555593313176,["28447@DESKTOP-BE7HQ26",[[1,3091,". "]],[3091,3091],[3093,3093]]],[1555593315679,["28447@DESKTOP-BE7HQ26",[[1,3101,"\n"]],[3099,3099],[3100,3100]]],[1555593328006,["28447@DESKTOP-BE7HQ26",[[-1,2726,"15、"]],[2726,2729],[2726,2726]]],[1555593331324,["28447@DESKTOP-BE7HQ26",[[1,2724,"15. "]],[2724,2724],[2728,2728]]],[1555593338237,["28447@DESKTOP-BE7HQ26",[[1,3103,"\n"]],[3100,3100],[3101,3101]]],[1555593365915,["28447@DESKTOP-BE7HQ26",[[1,3101,"![计算流程]($resource/%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B.jpg)"]],[3101,3101],[3161,3161]]],[1555593386198,["28447@DESKTOP-BE7HQ26",[[1,3163,"\n"]],[3161,3161],[3162,3162]]],[1555593449042,["28447@DESKTOP-BE7HQ26",[[1,3161,"**2、从代码构建DAG图**\n\nSpark program\nVal lines1 = sc.textFile(inputPath1). map(···)). map(···)\nVal lines2 = sc.textFile(inputPath2) . map(···)\nVal lines3 = sc.textFile(inputPath3)\nVal dtinone1 = lines2.union(lines3)\nVal dtinone = lines1.join(dtinone1)\ndtinone.saveAsTextFile(···)\ndtinone.filter(···).foreach(···)\n\nSpark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。\n\nSpark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。"]],[3161,3161],[3595,3595]]],[1555593451453,["28447@DESKTOP-BE7HQ26",[[1,3161,"\n"]],[3160,3160],[3161,3161]]],[1555593455764,["28447@DESKTOP-BE7HQ26",[[1,3599,"\n"]],[3596,3596],[3597,3597]]],[1555593456404,["28447@DESKTOP-BE7HQ26",[[1,3600,"\n"]],[3597,3597],[3598,3598]]],[1555593494829,["28447@DESKTOP-BE7HQ26",[[1,3598,"![构建DAG图]($resource/%E6%9E%84%E5%BB%BADAG%E5%9B%BE.jpg)"]],[3598,3598],[3654,3654]]],[1555595397630,["28447@DESKTOP-BE7HQ26",[[1,3656,"\n"]],[3653,3653],[3654,3654]]],[1555595397828,["28447@DESKTOP-BE7HQ26",[[1,3657,"\n"]],[3654,3654],[3655,3655]]],[1555595398261,["28447@DESKTOP-BE7HQ26",[[1,3655,"**3、将DAG划分为Stage核心算法**\n\nApplication多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。\n\n划分依据：Stage划分的依据就是宽依赖，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。\n\n核心算法：从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。"]],[3655,3655],[4132,4132]]],[1555595406999,["28447@DESKTOP-BE7HQ26",[[-1,3851,"Stage划分的依据就是宽依赖"]],[3851,3866],[3851,3851]]],[1555595408534,["28447@DESKTOP-BE7HQ26",[[1,3851,"****"]],[3851,3851],[3853,3853]]],[1555595408796,["28447@DESKTOP-BE7HQ26",[[1,3853,"Stage划分的依据就是宽依赖"]],[3853,3853],[3868,3868]]],[1555595430255,["28447@DESKTOP-BE7HQ26",[[-1,3923,"从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分"]],[3923,3958],[3923,3923]]],[1555595431667,["28447@DESKTOP-BE7HQ26",[[1,3923,"****"]],[3923,3923],[3925,3925]]],[1555595432014,["28447@DESKTOP-BE7HQ26",[[1,3925,"从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分"]],[3925,3925],[3960,3960]]],[1555595434496,["28447@DESKTOP-BE7HQ26",[[1,4143,"\n"]],[4140,4140],[4141,4141]]],[1555595434785,["28447@DESKTOP-BE7HQ26",[[1,4144,"\n"]],[4141,4141],[4142,4142]]],[1555595563937,["28447@DESKTOP-BE7HQ26",[[1,4142,"**6、提交Stages**\n\n调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说，提交调度阶段的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算。"]],[4142,4142],[4386,4386]]],[1555595605774,["28447@DESKTOP-BE7HQ26",[[1,4387,"![提交stage]($resource/%E6%8F%90%E4%BA%A4stage.jpg)"]],[4387,4387],[4437,4437]]],[1555595607103,["28447@DESKTOP-BE7HQ26",[[1,4438,"\n"]],[4437,4437],[4438,4438]]],[1555595607342,["28447@DESKTOP-BE7HQ26",[[1,4439,"\n"]],[4438,4438],[4439,4439]]],[1555595868879,["28447@DESKTOP-BE7HQ26",[[1,4438,"**8、监控Job、Task、Executor**\n\nDAGScheduler监控Job与Task：要保证相互依赖的作业调度阶段能够得到顺利的调度执行，DAGScheduler需要监控当前作业调度阶段乃至任务的完成情况。这通过对外暴露一系列的回调函数来实现的，对于TaskScheduler来说，这些回调函数主要包括任务的开始结束失败、任务集的失败，DAGScheduler根据这些任务的生命周期信息进一步维护作业和调度阶段的状态信息。\n\nDAGScheduler监控Executor的生命状态：TaskScheduler通过回调函数通知DAGScheduler具体的Executor的生命状态，如果某一个Executor崩溃了，则对应的调度阶段任务集的ShuffleMapTask的输出结果也将标志为不可用，这将导致对应任务集状态的变更，进而重新执行相关计算任务，以获取丢失的相关数据。\n\n**9、获取任务执行结果**\n\n结果DAGScheduler：一个具体的任务在Executor中执行完毕后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。\n\n两种结果，中间结果与最终结果：对于FinalStage所对应的任务，返回给DAGScheduler的是运算结果本身，而对于中间调度阶段对应的任务ShuffleMapTask，返回给DAGScheduler的是一个MapStatus里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。\n\n两种类型，DirectTaskResult与IndirectTaskResult：根据任务结果大小的不同，ResultTask返回的结果又分为两类，如果结果足够小，则直接放在DirectTaskResult对象内中，如果超过特定尺寸则在Executor端会将DirectTaskResult先序列化，再把序列化的结果作为一个数据块存放在BlockManager中，然后将BlockManager返回的BlockID放在IndirectTaskResult对象中返回给TaskScheduler，TaskScheduler进而调用TaskResultGetter将IndirectTaskResult中的BlockID取出并通过BlockManager最终取得对应的DirectTaskResult。"]],[4438,4438],[5452,5452]]],[1555595872934,["28447@DESKTOP-BE7HQ26",[[1,5454,"\n"]],[5453,5453],[5454,5454]]],[1555595873399,["28447@DESKTOP-BE7HQ26",[[1,5455,"\n"]],[5454,5454],[5455,5455]]],[1555595883122,["28447@DESKTOP-BE7HQ26",[[1,5454,"**10、任务调度总体诠释**"]],[5454,5454],[5469,5469]]],[1555595884078,["28447@DESKTOP-BE7HQ26",[[1,5471,"\n"]],[5469,5469],[5470,5470]]],[1555595893607,["28447@DESKTOP-BE7HQ26",[[1,5470,"![任务调度总体诠释]($resource/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%80%BB%E4%BD%93%E8%AF%A0%E9%87%8A.jpg)"]],[5470,5470],[5570,5570]]],[1555595896676,["28447@DESKTOP-BE7HQ26",[[1,5470,"\n"]],[5469,5469],[5470,5470]]],[1555596018576,["28447@DESKTOP-BE7HQ26",[[1,11,"[]"]],[11,11],[13,13]]],[1555596022076,["28447@DESKTOP-BE7HQ26",[[1,12,"参考文章"]],[12,12],[16,16]]],[1555596024304,["28447@DESKTOP-BE7HQ26",[[-1,12,"参考文章"]],[16,16],[12,12]]],[1555596026162,["28447@DESKTOP-BE7HQ26",[[1,12,"Spark"]],[12,12],[17,17]]],[1555596029619,["28447@DESKTOP-BE7HQ26",[[1,17,"参考文章"]],[17,17],[21,21]]],[1555596031275,["28447@DESKTOP-BE7HQ26",[[1,22,"("]],[22,22],[23,23]]],[1555596031309,["28447@DESKTOP-BE7HQ26",[[1,23,"https://)"]],[23,23],[23,31]]],[1555596032223,["28447@DESKTOP-BE7HQ26",[[1,31,"blog.csdn.net/liuxiangke0210/article/details/79687240"]],[23,31],[84,84]]],[1555596034956,["28447@DESKTOP-BE7HQ26",[[-1,17,"参考文章"],[1,21,"y"]],[17,21],[18,18]]],[1555596035935,["28447@DESKTOP-BE7HQ26",[[1,18,"uanli "]],[18,18],[24,24]]],[1555596037900,["28447@DESKTOP-BE7HQ26",[[-1,17,"yuanli "]],[24,24],[17,17]]],[1555596044843,["28447@DESKTOP-BE7HQ26",[[1,17,"原理精解"]],[17,17],[21,21]]]],null,"28447@DESKTOP-BE7HQ26"],["db193b68-7bcb-4375-b17e-31b89e831f6e",1555741392870,"# Spark整理\n\n[Spark原理精解](https://blog.csdn.net/liuxiangke0210/article/details/79687240)\n## Spark基本概念\n1. **Application：Spark应用程序**\n    - 指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。\n    \n![Application]($resource/Application.jpg)\n2. **Driver：驱动程序**\n    - Spark中的Driver即运行上述Application的Main()函数并且创建SparkContext，其中创建SparkContext的目的是**为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager通信，进行资源的申请、任务的分配和监控等**;当Executor部分运行完毕后，Driver负责将SparkContext关闭。通常SparkContext代表Driver，如下图所示: \n    ![2.jpg](http://www.raincent.com/uploadfile/2018/0320/20180320013155532.jpg)\n    \n3. **Cluster Manager：资源管理器**\n  -  指的是在集群上获取资源的外部服务，常用的有：**Standalone**，Spark原生的资源管理器，由Master负责资源的分配;**Hadoop Yarn**，由Yarn中的ResearchManager负责资源的分配;Messos，由Messos中的Messos Master负责资源管理，如下图所示:\n  \n![Cluster Manager]($resource/Cluster%20Manager.jpg)\n\n4. **Executor：执行器**\n       Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立的一批Executor，如下图所示:\n  \n![Executor]($resource/Executor.jpg)\n\n5. **Worker：计算节点**\n        集群中任何可以运行Application代码的节点，类似于Yarn中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示:\n\n![Worker]($resource/Worker.jpg)\n\n6. **RDD：弹性分布式数据集**\nResillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作(主要有Transformation和Action操作)，如下图所示：\n\n![RDD]($resource/RDD.jpg)\n\n7. **父RDD每一个分区最多被一个子RDD的分区所用**;表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示:\n\n![窄依赖]($resource/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg)\n\n8. **宽依赖**\n\n父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示:\n\n![宽依赖]($resource/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg)\n\n常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join(父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖)。\n\n常见的宽依赖有groupByKey、partitionBy、reduceByKey、join(父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖)。\n\n9. **DAG：有向无环图**\nDirected Acycle graph，反应RDD之间的依赖关系，如图所示:\n![DAG]($resource/DAG.jpg)\n\n10. **DAGScheduler：有向无环图调度器**\n基于DAG划分Stage 并以TaskSet的形式提交Stage给TaskScheduler;负责将作业拆分成不同阶段的具有依赖关系的多批任务;最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在SparkContext初始化的过程中被实例化，一个SparkContext对应创建一个DAGScheduler。\n![DAGScheduler]($resource/DAGScheduler.jpg)\n\n11. **TaskScheduler：任务调度器**\n将Taskset提交给worker(集群)运行并回报结果;负责每个具体任务的实际物理调度。如图所示:\n\n![TaskScheduler]($resource/TaskScheduler.jpg)\n\n12. **Job：作业**\n由一个或多个调度阶段所组成的一次计算作业;包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示:\n\n![Job]($resource/Job.jpg)\n\n13. **Stage：调度阶段**\n\n一个任务集对应的调度阶段;每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段;Stage分成两种类型ShuffleMapStage、ResultStage。如图所示:\n![Stage]($resource/Stage.jpg)\n\n14. **TaskSet：任务集**\n\n由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示:\n![TaskSet]($resource/TaskSet.jpg)\n\n提示：\n\n1)一个Stage创建一个TaskSet;\n2)为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet\n\n15. **Task：任务**\n\n被送到某个Executor上的工作任务;单个分区数据集上的最小处理流程单元。如图所示:\n\n![Task]($resource/Task.jpg)\n\n总体如图所示：\n![总体图]($resource/%E6%80%BB%E4%BD%93%E5%9B%BE.jpg)\n\n## Spark运行流程\n![Spark运行基本流程1]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B1.jpg)\n\n![Spark运行基本流程2]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B2.jpg)\n\n## Spark核心原理透视\n\n**1. 计算流程**\n![计算流程]($resource/%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B.jpg)\n\n**2、从代码构建DAG图**\n\nSpark program\nVal lines1 = sc.textFile(inputPath1). map(···)). map(···)\nVal lines2 = sc.textFile(inputPath2) . map(···)\nVal lines3 = sc.textFile(inputPath3)\nVal dtinone1 = lines2.union(lines3)\nVal dtinone = lines1.join(dtinone1)\ndtinone.saveAsTextFile(···)\ndtinone.filter(···).foreach(···)\n\nSpark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。\n\nSpark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。\n\n![构建DAG图]($resource/%E6%9E%84%E5%BB%BADAG%E5%9B%BE.jpg)\n\n**3、将DAG划分为Stage核心算法**\n\nApplication多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。\n\n划分依据：**Stage划分的依据就是宽依赖**，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。\n\n核心算法：**从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分**。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。\n\n**6、提交Stages**\n\n调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说，提交调度阶段的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算。\n![提交stage]($resource/%E6%8F%90%E4%BA%A4stage.jpg)\n\n**8、监控Job、Task、Executor**\n\nDAGScheduler监控Job与Task：要保证相互依赖的作业调度阶段能够得到顺利的调度执行，DAGScheduler需要监控当前作业调度阶段乃至任务的完成情况。这通过对外暴露一系列的回调函数来实现的，对于TaskScheduler来说，这些回调函数主要包括任务的开始结束失败、任务集的失败，DAGScheduler根据这些任务的生命周期信息进一步维护作业和调度阶段的状态信息。\n\nDAGScheduler监控Executor的生命状态：TaskScheduler通过回调函数通知DAGScheduler具体的Executor的生命状态，如果某一个Executor崩溃了，则对应的调度阶段任务集的ShuffleMapTask的输出结果也将标志为不可用，这将导致对应任务集状态的变更，进而重新执行相关计算任务，以获取丢失的相关数据。\n\n**9、获取任务执行结果**\n\n结果DAGScheduler：一个具体的任务在Executor中执行完毕后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。\n\n两种结果，中间结果与最终结果：对于FinalStage所对应的任务，返回给DAGScheduler的是运算结果本身，而对于中间调度阶段对应的任务ShuffleMapTask，返回给DAGScheduler的是一个MapStatus里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。\n\n两种类型，DirectTaskResult与IndirectTaskResult：根据任务结果大小的不同，ResultTask返回的结果又分为两类，如果结果足够小，则直接放在DirectTaskResult对象内中，如果超过特定尺寸则在Executor端会将DirectTaskResult先序列化，再把序列化的结果作为一个数据块存放在BlockManager中，然后将BlockManager返回的BlockID放在IndirectTaskResult对象中返回给TaskScheduler，TaskScheduler进而调用TaskResultGetter将IndirectTaskResult中的BlockID取出并通过BlockManager最终取得对应的DirectTaskResult。\n\n**10、任务调度总体诠释**\n\n![任务调度总体诠释]($resource/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%80%BB%E4%BD%93%E8%AF%A0%E9%87%8A.jpg)\n\n",[[1555741384976,["28447@DESKTOP-BE7HQ26",[[1,3939,"shi"]],[3939,3939],[3942,3942]]],[1555741385622,["28447@DESKTOP-BE7HQ26",[[-1,3939,"shi"]],[3942,3942],[3939,3939]]],[1555741388442,["28447@DESKTOP-BE7HQ26",[[1,3939,"是否存在"]],[3939,3939],[3943,3943]]],[1555741406240,["28447@DESKTOP-BE7HQ26",[[1,3943,"ShuffleDependency"]],[3943,3943],[3960,3960]]],[1555758860224,["28447@DESKTOP-BE7HQ26",[[1,250,"（）"]],[250,250],[252,252]]],[1555758863387,["28447@DESKTOP-BE7HQ26",[[1,251,"Sparl"]],[251,251],[256,256]]],[1555758864127,["28447@DESKTOP-BE7HQ26",[[-1,255,"l"]],[256,256],[255,255]]],[1555758866901,["28447@DESKTOP-BE7HQ26",[[1,255,"kSubmit"]],[255,255],[262,262]]],[1555758870005,["28447@DESKTOP-BE7HQ26",[[1,251,"y"]],[251,251],[252,252]]],[1555758871015,["28447@DESKTOP-BE7HQ26",[[-1,251,"y"]],[252,252],[251,251]]],[1555758874303,["28447@DESKTOP-BE7HQ26",[[1,251,"也叫做"]],[251,251],[254,254]]]],null,"28447@DESKTOP-BE7HQ26"],["27c87577-1027-420f-a4cf-a862ac8d7a9b",1555858477397,"# Spark整理\n\n[Spark原理精解](https://blog.csdn.net/liuxiangke0210/article/details/79687240)\n## Spark基本概念\n1. **Application：Spark应用程序**\n    - 指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。\n    \n![Application]($resource/Application.jpg)\n2. **Driver（也叫做SparkSubmit）：驱动程序**\n    - Spark中的Driver即运行上述Application的Main()函数并且创建SparkContext，其中创建SparkContext的目的是**为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager通信，进行资源的申请、任务的分配和监控等**;当Executor部分运行完毕后，Driver负责将SparkContext关闭。通常SparkContext代表Driver，如下图所示: \n    ![2.jpg](http://www.raincent.com/uploadfile/2018/0320/20180320013155532.jpg)\n    \n3. **Cluster Manager：资源管理器**\n  -  指的是在集群上获取资源的外部服务，常用的有：**Standalone**，Spark原生的资源管理器，由Master负责资源的分配;**Hadoop Yarn**，由Yarn中的ResearchManager负责资源的分配;Messos，由Messos中的Messos Master负责资源管理，如下图所示:\n  \n![Cluster Manager]($resource/Cluster%20Manager.jpg)\n\n4. **Executor：执行器**\n       Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立的一批Executor，如下图所示:\n  \n![Executor]($resource/Executor.jpg)\n\n5. **Worker：计算节点**\n        集群中任何可以运行Application代码的节点，类似于Yarn中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示:\n\n![Worker]($resource/Worker.jpg)\n\n6. **RDD：弹性分布式数据集**\nResillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作(主要有Transformation和Action操作)，如下图所示：\n\n![RDD]($resource/RDD.jpg)\n\n7. **父RDD每一个分区最多被一个子RDD的分区所用**;表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示:\n\n![窄依赖]($resource/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg)\n\n8. **宽依赖**\n\n父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示:\n\n![宽依赖]($resource/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg)\n\n常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join(父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖)。\n\n常见的宽依赖有groupByKey、partitionBy、reduceByKey、join(父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖)。\n\n9. **DAG：有向无环图**\nDirected Acycle graph，反应RDD之间的依赖关系，如图所示:\n![DAG]($resource/DAG.jpg)\n\n10. **DAGScheduler：有向无环图调度器**\n基于DAG划分Stage 并以TaskSet的形式提交Stage给TaskScheduler;负责将作业拆分成不同阶段的具有依赖关系的多批任务;最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在SparkContext初始化的过程中被实例化，一个SparkContext对应创建一个DAGScheduler。\n![DAGScheduler]($resource/DAGScheduler.jpg)\n\n11. **TaskScheduler：任务调度器**\n将Taskset提交给worker(集群)运行并回报结果;负责每个具体任务的实际物理调度。如图所示:\n\n![TaskScheduler]($resource/TaskScheduler.jpg)\n\n12. **Job：作业**\n由一个或多个调度阶段所组成的一次计算作业;包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示:\n\n![Job]($resource/Job.jpg)\n\n13. **Stage：调度阶段**\n\n一个任务集对应的调度阶段;每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段;Stage分成两种类型ShuffleMapStage、ResultStage。如图所示:\n![Stage]($resource/Stage.jpg)\n\n14. **TaskSet：任务集**\n\n由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示:\n![TaskSet]($resource/TaskSet.jpg)\n\n提示：\n\n1)一个Stage创建一个TaskSet;\n2)为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet\n\n15. **Task：任务**\n\n被送到某个Executor上的工作任务;单个分区数据集上的最小处理流程单元。如图所示:\n\n![Task]($resource/Task.jpg)\n\n总体如图所示：\n![总体图]($resource/%E6%80%BB%E4%BD%93%E5%9B%BE.jpg)\n\n## Spark运行流程\n![Spark运行基本流程1]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B1.jpg)\n\n![Spark运行基本流程2]($resource/Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B2.jpg)\n\n## Spark核心原理透视\n\n**1. 计算流程**\n![计算流程]($resource/%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B.jpg)\n\n**2、从代码构建DAG图**\n\nSpark program\nVal lines1 = sc.textFile(inputPath1). map(···)). map(···)\nVal lines2 = sc.textFile(inputPath2) . map(···)\nVal lines3 = sc.textFile(inputPath3)\nVal dtinone1 = lines2.union(lines3)\nVal dtinone = lines1.join(dtinone1)\ndtinone.saveAsTextFile(···)\ndtinone.filter(···).foreach(···)\n\nSpark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。\n\nSpark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。\n\n![构建DAG图]($resource/%E6%9E%84%E5%BB%BADAG%E5%9B%BE.jpg)\n\n**3、将DAG划分为Stage核心算法**\n\nApplication多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。\n\n划分依据：**Stage划分的依据就是是否存在ShuffleDependency宽依赖**，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。\n\n核心算法：**从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分**。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。\n\n**6、提交Stages**\n\n调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说，提交调度阶段的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算。\n![提交stage]($resource/%E6%8F%90%E4%BA%A4stage.jpg)\n\n**8、监控Job、Task、Executor**\n\nDAGScheduler监控Job与Task：要保证相互依赖的作业调度阶段能够得到顺利的调度执行，DAGScheduler需要监控当前作业调度阶段乃至任务的完成情况。这通过对外暴露一系列的回调函数来实现的，对于TaskScheduler来说，这些回调函数主要包括任务的开始结束失败、任务集的失败，DAGScheduler根据这些任务的生命周期信息进一步维护作业和调度阶段的状态信息。\n\nDAGScheduler监控Executor的生命状态：TaskScheduler通过回调函数通知DAGScheduler具体的Executor的生命状态，如果某一个Executor崩溃了，则对应的调度阶段任务集的ShuffleMapTask的输出结果也将标志为不可用，这将导致对应任务集状态的变更，进而重新执行相关计算任务，以获取丢失的相关数据。\n\n**9、获取任务执行结果**\n\n结果DAGScheduler：一个具体的任务在Executor中执行完毕后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。\n\n两种结果，中间结果与最终结果：对于FinalStage所对应的任务，返回给DAGScheduler的是运算结果本身，而对于中间调度阶段对应的任务ShuffleMapTask，返回给DAGScheduler的是一个MapStatus里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。\n\n两种类型，DirectTaskResult与IndirectTaskResult：根据任务结果大小的不同，ResultTask返回的结果又分为两类，如果结果足够小，则直接放在DirectTaskResult对象内中，如果超过特定尺寸则在Executor端会将DirectTaskResult先序列化，再把序列化的结果作为一个数据块存放在BlockManager中，然后将BlockManager返回的BlockID放在IndirectTaskResult对象中返回给TaskScheduler，TaskScheduler进而调用TaskResultGetter将IndirectTaskResult中的BlockID取出并通过BlockManager最终取得对应的DirectTaskResult。\n\n**10、任务调度总体诠释**\n\n![任务调度总体诠释]($resource/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%80%BB%E4%BD%93%E8%AF%A0%E9%87%8A.jpg)\n\n",[[1555858427107,["28447@DESKTOP-BE7HQ26",[[1,5683,"\n"]],[5681,5681],[5682,5682]]],[1555858427274,["28447@DESKTOP-BE7HQ26",[[1,5684,"\n"]],[5682,5682],[5683,5683]]],[1555858430475,["28447@DESKTOP-BE7HQ26",[[1,5683,"## Sparl"]],[5683,5683],[5691,5691]]],[1555858430980,["28447@DESKTOP-BE7HQ26",[[-1,5690,"l"]],[5691,5691],[5690,5690]]],[1555858433441,["28447@DESKTOP-BE7HQ26",[[1,5690,"k shffle"]],[5690,5690],[5698,5698]]],[1555858435004,["28447@DESKTOP-BE7HQ26",[[-1,5694,"ffle"]],[5698,5698],[5694,5694]]],[1555858439245,["28447@DESKTOP-BE7HQ26",[[1,5694,"uffle过程"]],[5694,5694],[5701,5701]]],[1555858439819,["28447@DESKTOP-BE7HQ26",[[1,5703,"\n"]],[5701,5701],[5702,5702]]],[1555858456628,["28447@DESKTOP-BE7HQ26",[[1,5702,"shufflefa'sheng'z\\"]],[5702,5702],[5720,5720]]],[1555858458868,["28447@DESKTOP-BE7HQ26",[[-1,5709,"fa'sheng'z\\"]],[5720,5720],[5709,5709]]],[1555858469392,["28447@DESKTOP-BE7HQ26",[[1,5709,"发生在stage"]],[5709,5709],[5717,5717]]],[1555858475989,["28447@DESKTOP-BE7HQ26",[[1,5712,"两个"]],[5712,5712],[5714,5714]]],[1555858478901,["28447@DESKTOP-BE7HQ26",[[1,5719,"之间"]],[5719,5719],[5721,5721]]],[1555858479692,["28447@DESKTOP-BE7HQ26",[[1,5723,"\n"]],[5721,5721],[5722,5722]]],[1555858479900,["28447@DESKTOP-BE7HQ26",[[1,5724,"\n"]],[5722,5722],[5723,5723]]],[1555858480083,["28447@DESKTOP-BE7HQ26",[[1,5725,"\n"]],[5723,5723],[5724,5724]]]],null,"28447@DESKTOP-BE7HQ26"]]}